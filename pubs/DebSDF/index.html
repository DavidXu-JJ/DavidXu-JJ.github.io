<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DebSDF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://davidxu-jj.github.io/projects/DebSDF">
    <meta property="og:title" content="DebSDF: Delving into the Details and Bias of Neural Indoor Scene Reconstruction">
    <meta property="og:description" content="In recent years, the neural implicit surface has emerged as a powerful representation for multi-view surface reconstruction due to its simplicity and state-of-the-art performance. However, reconstructing smooth and detailed surfaces in indoor scenes from multi-view images presents unique challenges. Indoor scenes typically contain large texture-less regions, making the photometric loss unreliable for optimizing the implicit surface. Previous work utilizes monocular geometry priors to improve the reconstruction in indoor scenes. However, monocular priors often contain substantial errors in thin structure regions due to domain gaps and the inherent inconsistencies when derived independently from different views. This paper presents DebSDF to address these challenges, focusing on the utilization of uncertainty in monocular priors. We propose an uncertainty modeling technique that associates larger uncertainties with larger errors in the priors. High-uncertainty priors are then excluded from optimization to prevent bias. This uncertainty measure also informs an importance-guided ray sampling and adaptive smoothness regularization, enhancing the learning of fine structures. We further introduce a bias-aware signed distance to density transformation that takes into account the curvature radius and the angle between the view direction and the SDF normals to better reconstruct fine details. Our approach has been validated through extensive experiments on several challenging datasets, demonstrating improved qualitative and quantitative results in reconstructing thin structures in indoor scenes, thereby outperforming previous work. The code will be made publicly available.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="DebSDF: Delving into the Details and Bias of Neural Indoor Scene Reconstruction">
    <meta name="twitter:description" content="In recent years, the neural implicit surface has emerged as a powerful representation for multi-view surface reconstruction due to its simplicity and state-of-the-art performance. However, reconstructing smooth and detailed surfaces in indoor scenes from multi-view images presents unique challenges. Indoor scenes typically contain large texture-less regions, making the photometric loss unreliable for optimizing the implicit surface. Previous work utilizes monocular geometry priors to improve the reconstruction in indoor scenes. However, monocular priors often contain substantial errors in thin structure regions due to domain gaps and the inherent inconsistencies when derived independently from different views. This paper presents DebSDF to address these challenges, focusing on the utilization of uncertainty in monocular priors. We propose an uncertainty modeling technique that associates larger uncertainties with larger errors in the priors. High-uncertainty priors are then excluded from optimization to prevent bias. This uncertainty measure also informs an importance-guided ray sampling and adaptive smoothness regularization, enhancing the learning of fine structures. We further introduce a bias-aware signed distance to density transformation that takes into account the curvature radius and the angle between the view direction and the SDF normals to better reconstruct fine details. Our approach has been validated through extensive experiments on several challenging datasets, demonstrating improved qualitative and quantitative results in reconstructing thin structures in indoor scenes, thereby outperforming previous work. The code will be made publicly available.">
    <meta name="twitter:image" content="">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>DebSDF</b>: Delving into the Details and Bias of <br> Neural Indoor Scene Reconstruction<br>
                <small>
                (coming soon)
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://svip-lab.github.io/team/xiaoyt.html">
                              Yuting Xiao
                              <span class="text-span_star">*</span>
                            </a>
                            <br> ShanghaiTech University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://davidxu-jj.github.io/">
                              Jingwei Xu
                              <span class="text-span_star">*</span>
                            </a>
                            <br> ShanghaiTech University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://niujinshuchong.github.io">
                              Zehao Yu
                            </a>
                            <br>University of Tübingen
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://svip-lab.github.io/team.html">
                              Shenghua Gao
                            </a>
                            <br> ShanghaiTech University
                        </td>
                    </tr>
                </table>
            </div>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <br>
            <span class="text-span_star">*</span> Denotes Equal Contribution
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <img src="./img/paper_image.jpg" height="60px">
                                <h4><strong>Paper(coming soon)</strong></h4>
                            </a>
                        </li>
                        <!-- <li> -->
                        <!--     <a href="https://youtu.be/qrdRH9irAlk"> -->
                        <!--     <img src="./img/youtube_icon.png" height="60px"> -->
                        <!--         <h4><strong>Video</strong></h4> -->
                        <!--     </a> -->
                        <!-- </li> -->
                        <!-- <li> -->
                        <!--     <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank"> -->
                        <!--     <image src="img/database_icon.png" height="60px"> -->
                        <!--         <h4><strong>Shiny Dataset</strong></h4> -->
                        <!--     </a> -->
                        <!-- </li> -->
                        <!-- <li> -->
                        <!--     <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank"> -->
                        <!--     <image src="img/real_database_icon.png" height="60px"> -->
                        <!--         <h4><strong>Real Dataset</strong></h4> -->
                        <!--     </a> -->
                        <!-- </li>                             -->
                        <li>
                            <a href="" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code(coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <!-- <div class="row"> -->
        <!--     <div class="col-md-8 col-md-offset-2"> -->
        <!--         <div class="video-compare-container" id="materialsDiv"> -->
        <!--             <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video> -->
        <!---->
        <!--             <canvas height=0 class="videoMerge" id="materialsMerge"></canvas> -->
        <!--         </div> -->
        <!--   </div> -->
        <!-- </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                In recent years, the neural implicit surface has emerged as a powerful representation for multi-view surface reconstruction due to its simplicity and state-of-the-art performance. However, reconstructing smooth and detailed surfaces in indoor scenes from multi-view images presents unique challenges. Indoor scenes typically contain large texture-less regions, making the photometric loss unreliable for optimizing the implicit surface. Previous work utilizes monocular geometry priors to improve the reconstruction in indoor scenes. However, monocular priors often contain substantial errors in thin structure regions due to domain gaps and the inherent inconsistencies when derived independently from different views. This paper presents <b>DebSDF</b> to address these challenges, focusing on the utilization of uncertainty in monocular priors. We propose an uncertainty modeling technique that associates larger uncertainties with larger errors in the priors. High-uncertainty priors are then excluded from optimization to prevent bias. This uncertainty measure also informs an importance-guided ray sampling and adaptive smoothness regularization, enhancing the learning of fine structures. We further introduce a bias-aware signed distance to density transformation that takes into account the curvature radius and the angle between the view direction and the SDF normals to better reconstruct fine details. Our approach has been validated through extensive experiments on several challenging datasets, demonstrating improved qualitative and quantitative results in reconstructing thin structures in indoor scenes, thereby outperforming previous work. The code will be made publicly available.
                </p>
            </div>
        </div>

        <image src="img/teaser.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Architecture
                </h3>
            </div>
        </div>

        <image src="img/architecture.png" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <!-- <div class="row"> -->
        <!--     <div class="col-md-8 col-md-offset-2"> -->
        <!--         <h3> -->
        <!--             Video -->
        <!--         </h3> -->
        <!--         <div class="text-center"> -->
        <!--             <div style="position:relative;padding-top:56.25%;"> -->
        <!--                 <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
        <!--             </div> -->
        <!--         </div> -->
        <!--     </div> -->
        <!-- </div> -->
        <!---->
        <!-- <div class="row"> -->
        <!--     <div class="col-md-8 col-md-offset-2"> -->
        <!--         <h3> -->
        <!--             Reflection Direction Parameterization -->
        <!--         </h3> -->
        <!--         <div class="text-justify"> -->
        <!--             Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results. -->
        <!---->
        <!--             <br><br> -->
        <!---->
        <!--         </div> -->
        <!--         <div class="text-center"> -->
        <!--             <video id="refdir" width="40%" playsinline autoplay loop muted> -->
        <!--                 <source src="video/reflection_animation.mp4" type="video/mp4" /> -->
        <!--             </video> -->
        <!--         </div> -->
        <!--     </div> -->
        <!-- </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                  Reconstruction Comparison
                </h3>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/scannet4.mp4" type="video/mp4" />
                    </video>
                </div>
                
                <br>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/rgbd1.mp4" type="video/mp4" />
                    </video>
                </div>

                <br>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/rgbd3.mp4" type="video/mp4" />
                    </video>
                </div>

                <br>
                <div class="text-center">
                    <video id="ide" width="100%" playsinline autoplay controls loop muted>
                        <source src="video/rgbd5.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        </div>

			<!--      <div class="row"> -->
			<!--          <div class="col-md-8 col-md-offset-2"> -->
			<!--              <h3> -->
			<!--                  Additional Synthetic Results -->
			<!--              </h3> -->
			<!--              <div class="video-compare-container"> -->
			<!--                  <video class="video" id="musclecar" loop playsinline autoPlay muted src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video> -->
			<!--                  <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas> -->
			<!--              </div> -->
			<!-- </div> -->
			<!--      </div> -->
			<!---->
			<!--      <div class="row"> -->
			<!--          <div class="col-md-8 col-md-offset-2"> -->
			<!--              <h3> -->
			<!--                  Results on Captured Scenes -->
			<!--              </h3> -->
			<!--              Our method also produces accurate renderings and surface normals from captured photographs: -->
			<!--              <div class="video-compare-container" style="width: 100%"> -->
			<!--                  <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video> -->
			<!--                  <canvas height=0 class="videoMerge" id="toycarMerge"></canvas> -->
			<!--              </div> -->
			<!-- </div> -->
			<!--      </div> -->
			<!---->
			<!--      <div class="row"> -->
			<!--          <div class="col-md-8 col-md-offset-2"> -->
			<!--              <h3> -->
			<!--                  Scene Editing -->
			<!--              </h3> -->
			<!--              <div class="text-justify"> -->
			<!--                  We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties. -->
			<!--                  <br> -->
			<!--                  We can increase and decrease material roughness: -->
			<!--              </div> -->
			<!---->
			<!--              <div style="overflow: hidden;"> -->
			<!--                  <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;"> -->
			<!--                      <source src="video/materials_rougher_smoother.mp4" type="video/mp4" /> -->
			<!--                  </video> -->
			<!--              </div> -->
			<!---->
			<!--              <div class="text-justify"> -->
			<!--                  We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections: -->
			<!--              </div> -->
			<!---->
			<!--              <table width="100%"> -->
			<!--                  <tr> -->
			<!--                      <td align="left" valign="top" width="50%"> -->
			<!--                          <video id="v2" width="100%" playsinline autoplay loop muted> -->
			<!--                              <source src="video/car_color2.mp4" type="video/mp4" /> -->
			<!--                          </video> -->
			<!--                      </td> -->
			<!--                      <td align="left" valign="top" width="50%"> -->
			<!--                          <video id="v3" width="100%" playsinline autoplay loop muted> -->
			<!--                              <source src="video/car_color3.mp4" type="video/mp4" /> -->
			<!--                          </video> -->
			<!--                      </td> -->
			<!--                  </tr> -->
			<!--              </table> -->
			<!---->
			<!--          </div> -->
			<!--      </div> -->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                coming soon...
                    <textarea id="bibtex" class="form-control" readonly>
</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
